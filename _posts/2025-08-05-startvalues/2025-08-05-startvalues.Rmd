---
title: "Start values"
description: |
  Opening a can of worms...
author:
  - name: Jessica
categories:
  - research
date: 08-05-2025
output:
  distill::distill_article:
    self_contained: false
---

## Introduction


The importance of initializing the algorithm with many starting values has been written many times in [best practice guides](https://www.sciencedirect.com/science/article/pii/S0001879120300701?utm_source=chatgpt.com#s0010), but there is far less research about how exactly to generate those starting values. Expectation-Maximization (EM) algorithm is the most common estimation technique for latent class models, and it guarantees convergence to a local optima, but not necessarily a global optima. A local optima is the location of the solution with the highest value in a particular region, but not necessarily the highest value in the entire parameter space. Starting values are highly influential for where the algorithm will converge in mixture models. Therefore, there is a recommendation to use many different starting values so that we have a higher chance of finding the global optima rather than settling for a local optima. However, we should think about quality > quantity. If all of our starting values are bunched up in one corner of the parameter space, then we might be missing out on opportunities to explore other regions of the space that may have a higher value. Not all starting value generation methods were created equal. A [study](https://link.springer.com/article/10.3758/s13428-015-0697-6) done by Shireman et al. found that using k-means to find start values resulted in less local optima, while using a constrained EM technique found the best solution more often than other start value methods in most conditions. 

## Different Start Value Methods
- Random probabilities

- Random Assignment

- [Latin Hypercube Sampling?!](https://en.wikipedia.org/wiki/Latin_hypercube_sampling) The name makes the method sound very futuristic and advanced. It is supposedly a more efficient alternative to grid search. Say I have 2 parameters X and Y that both range from 0 to 1. If I want to take 10 random samples from the entire parameter space, I can divide both X and Y into 10 equal chunks (chunk 1: 0-0.1, chunk 2: 0.1-0.2, etc). Sample randomly from each chunk so that we will have a vector of 10 values for X and a vector of 10 values for Y. Shuffle the two vectors so that we have 10 pairs of (X,Y) starting values. This method supposedly covers the parameter space well and is less computationally intensive than grid search. For grid search we would have every single X,Y combination such as (0.01,0.01), (0.01,0.02), (0.01.0.03)...as starting values to use for our algorithm. With 2 parameters this could be done, but with 10 or even 44 parameters (like in the example I will show), this will blow up quickly. 


## What I did



## Results





