[
  {
    "path": "posts/2025-08-18-local-solutions/",
    "title": "Can we use local solutions to predict quality of latent class models?",
    "description": "One man's trash is another man's treasure",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-08-18",
    "categories": [
      "research"
    ],
    "contents": "\n\n\n\nIntroduction\nI recently submitted my master’s thesis! Yay! Today’s topic highlights one portion of my master’s thesis where I hope to diver deeper and make useful contributions. In my thesis I did a very initial exploration of the connection between presence of local solutions and parameter estimate quality. I will share what I did, some potential issues with my method, my next steps, and current roadblocks I have encountered. Let me be absolutely clear and say that this is no means a guide or a tutorial of any kind. This is all extremely exploratory and I don’t want anyone to start using local solutions to draw any conclusions (yet).\nWhen estimating latent class models under the maximum likelihood estimation (MLE) framework, we aim to obtain parameters that maximize the probability of our observed data occurring. Observed data are the responses we collected from actual participants on a specific set of questions/items. There are different algorithms under MLE that we can use to get item response probabilites and latent class prevelances, the parameters of interest if we are fitting latent class models. It is well-known that estimating latent class models can lead to convergence to local solutions. Theoretically, algorithms like Expectation-Maximization guarantee convergence to a local maximum, meaning a maximum of a particular region of the parameter space but not across the entire parameter space. This means that given a set of starting values, the algorithm may converge to an inferior solution that we do not want to interpret or use as our final solution. The recommendation is that many different sets of starting values should be used, and the solution corresponding to the largest maximum likelihood out of all of those sets is assumed to be the global maximum of the parameter space (even though we aren’t 100% sure this is true). Most researchers understand the importance of using many sets of starting values and examining the best solution. But what about the rest of the solutions that the algorithm converged to that were suboptimal? I call these local solutions, but they can go by many other names like local optima or modes. My impression is that researchers do not investigate or pay attention to these local solutions at all. One of the latent class model estimation packages in R, poLCA, will only show the solution and final likelihood of the iteration with the largest likelihood, even when you specify hundreds or thousands of repetitions.\nWhat if we could use information like the number of local solutions or the proportion of iterations that converged to the “global” maximima for a given set of repetitions as a predictor for latent class quality? In simulation studies it is possible to vary item response probability values or class prevalence balance, but with empirical data we will never know what the true values of our parameters are. We will also never know the true number of classes! If we can harness information that we get as a by-product of our estimation process, maybe it can help give us more confidence in our solution, or warn us of potential estimation issues. Hence the sub-header of this article: one man’s trash is another man’s treasure. If my research uncovers some kind of link between local solutions and estimation quality, then we can encourage researchers to examine their output more closely and not just focus on the maximum likelihood solution.\nInspiration from Shireman et. al (2016)\nI am not the first person to think of this idea. In the Oxford Handbook of Quantitative Methods, Masyn suggests that a poorly identified model will have a low level of maximum log likelihood solution replication and shows an example with a distribution of converged likelihood values. It is not clear to me what constitutes “low level” or “high level” of maximum likelihood solution replication. This lack of specificity might not be useful to researchers.\nIn a simulation study, Shireman et. al (2016) tried to establish a correlation between the number of local solutions, classification accuracy, and bias. They found that the proportion of local solutions was negatively correlated (-.5) with classification accuracy. They suggest that proportion of local solutions below .20 suggest “overall excellent solutions”. I draw a lot of inspiration for my research from this paper but I do think there is room for further research! I personally find that classification accuracy is not as important if the parameters you recover do not resemble the true class structure! I think bias should be examined closely first before focusing on classification accuracy. I also think when using local solutions to predict outcomes, we should also include any knowledge about the dataset or the latent class model itself. If we know the number of items and the sample size, will also knowing the number of local solutions or proportion of iterations converging to the maximum give us more insight about the bias of our parameter estimates?\nMy data\nAs I mentioned earlier, this line of research is just one portion of my master’s thesis. My overall thesis aimed to compare the estimation behavior of Expectation-Maximization algorithm and Newton Raphson. I generated 8 different simulation conditions with 3 factors:\n- Sample size (400 vs 1,000)\n- Class prevalence balance (balanced vs unbalanced)\n- Measurement error/item response probabilities (low vs high measurement error)\nI kept the number of classes (4) and the number of items (10) fixed for simplicity, but I am aware that varying this is super important to having generalizable results.\nMeasurement error refers to how far away item response probabilities are from 0 and 1. If an item response probability for a given class and item is .80, that means that there is a 20% chance that individuals in this class will have the other “incorrect” answer that is not characteristic of their class membership. This item could be interpreted as having low measurement error, compared to an item response probability of .60. In my simulation study, items in the high measurement error condition had probabilities .65/.35, while items in the low measurement error condition had probabilities .8/.2.\nClass prevalence balance refers to how big classes are. Unbalanced means that a large proportion of individuals are members of one class, while balanced means that there is more of an even spread among all of the classes. Unbalanced class prevalences are harder to estimate because there is less information for the algorithm for the smaller classes.\nAn example of the true parameters are shown below. This simulation condition has high measurement error and balanced latent class prevalences.\n\nTable 1: True Parameters for Simulation 1/2\nTrueParameters\nClass1\nClass2\nClass3\nClass4\nClass Prevalence\n0.35\n0.27\n0.24\n0.14\nItem 1\n0.65\n0.65\n0.35\n0.35\nItem 2\n0.65\n0.65\n0.35\n0.35\nItem 3\n0.65\n0.65\n0.35\n0.35\nItem 4\n0.65\n0.65\n0.35\n0.35\nItem 5\n0.65\n0.65\n0.35\n0.35\nItem 6\n0.65\n0.65\n0.65\n0.35\nItem 7\n0.65\n0.65\n0.65\n0.35\nItem 8\n0.65\n0.65\n0.65\n0.35\nItem 9\n0.65\n0.65\n0.65\n0.35\nItem 10\n0.65\n0.65\n0.65\n0.35\n\nFor each simulation condition I generated 6 datasets. I ran both Newton Raphson and Expectation Maximization using the same starting values (500 sets total). I collected a lot of information but for the purposes of this article I calculated the bias, number local solutions, and proportion of starting value sets that converged to the maximum likelihood solution. The number of local solutions and proportion of starting values that converged to the maximum are not directly inverses of each other, although they represent similar information. It is possible to have a small number of local solutions but also a low proportion of iterations converging to the maximum because many iterations are just converging to 1 or 2 other local solutions! For my thesis I used number of local solutions as a predictor of bias but here I will use proportion of iterations converging to the maximum.\nBuilding a linear model\nInstead of just examining the correlation between the proportion of iterations converging to the maximum and bias, I fit a linear model accounting for sample size and algorithm used (since I used two on the same dataset). In my thesis I actually fit a multi-level model using simulation number as the random intercept since I have 6 datasets per condition so observations are not independent. I think using a multilevel structure makes sense to produce accurate standard errors, but conceptually I’m not so sure I needed to include simulation number because it contains information that applied researchers would not have with their empirical datasets. In my original thesis I found that after accounting for simulation condition (which itself contains information about measurement error, class prevalence, and sample size), the number of local solutions did not contribute signficantly into the prediction of bias. However, we wouldn’t know the true measurement error of items or class prevalence with empirical datasets, so we downplay the contribution of the number of local solutions, which we can see!\n\n\nCall:\nlm(formula = bias ~ percMax + algorithm + factor(ss), data = df_long)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.08253 -0.02877 -0.01372  0.03108  0.08467 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     0.147540   0.009532  15.479  < 2e-16 ***\npercMax        -0.081221   0.011671  -6.959 4.93e-10 ***\nalgorithmNR     0.002459   0.008639   0.285   0.7766    \nfactor(ss)1000 -0.019775   0.008767  -2.256   0.0265 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04229 on 92 degrees of freedom\nMultiple R-squared:  0.3983,    Adjusted R-squared:  0.3786 \nF-statistic:  20.3 on 3 and 92 DF,  p-value: 3.526e-10\n\nThe summary of the linear model is shown above. The proportion of iterations converging to maximum is a signficantly associated with bias, along with the sample size. An increase in the proportion is associated with a decrease in bias.\n\n# Effect Size for ANOVA (Type I)\n\nParameter  | Eta2 (partial) |       95% CI\n------------------------------------------\npercMax    |           0.38 | [0.25, 1.00]\nalgorithm  |       9.84e-04 | [0.00, 1.00]\nfactor(ss) |           0.05 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\nLooking at the effect size of the linear model, the proportion of iterations converging to the maximum explains a lot of the variation in the bias compared to the algorithm and the sample size. This is interesting!\nInvestigating specific cases\nHowever, I’m not entirely convinced the relationship between proportion of convergence to the maximum and bias is entirely there. I enjoy looking at maximum likelihood estimates of my different datasets and comparing them to the truth. In this dataset, 81% of the starting values converged to the maximum, whose parameters are displayed on the right.\n\n\nTable 2: Comparing the Max Solution to the True Solution\n\n\nTrueParameters\nClass1\nClass2\nClass3\nClass4\nClass Prevalence\n0.35\n0.27\n0.24\n0.14\nItem 1\n0.65\n0.65\n0.35\n0.35\nItem 2\n0.65\n0.65\n0.35\n0.35\nItem 3\n0.65\n0.65\n0.35\n0.35\nItem 4\n0.65\n0.65\n0.35\n0.35\nItem 5\n0.65\n0.65\n0.35\n0.35\nItem 6\n0.65\n0.65\n0.65\n0.35\nItem 7\n0.65\n0.65\n0.65\n0.35\nItem 8\n0.65\n0.65\n0.65\n0.35\nItem 9\n0.65\n0.65\n0.65\n0.35\nItem 10\n0.65\n0.65\n0.65\n0.35\n\n\nMaxSolution\nClass1\nClass2\nClass3\nClass4\nClass Prevalence\n0.24\n0.34\n0.32\n0.10\nItem 1\n0.59\n0.56\n0.34\n0.56\nItem 2\n0.65\n0.52\n0.27\n0.64\nItem 3\n0.59\n0.74\n0.44\n0.48\nItem 4\n0.65\n0.93\n0.13\n0.24\nItem 5\n0.50\n0.53\n0.47\n0.55\nItem 6\n1.00\n0.31\n0.44\n0.39\nItem 7\n0.71\n0.47\n0.70\n0.14\nItem 8\n0.68\n0.56\n0.54\n0.11\nItem 9\n0.64\n0.45\n0.74\n0.11\nItem 10\n0.82\n0.43\n0.62\n0.06\n\n\nHowever, you can see how DIFFERENT it is compared to the true parameters! If you interpreted the maximum solution it would be completely different from the true class solutions. One cannot rely entirely on the proportion of convergence to the maximum. “Even if a thousand people say it, a wrong is still a wrong.”\nThe world’s largest caveat\nThere is something I have to admit: the number of local solutions or proportion of convergence to the maximum is ENTIRELY DEPENDENT ON HOW YOU GENERATED START VALUES! How you decide to generate start values (random draw values from a uniform, randomly assign individuals to initial classes, k-means, etc.) will decide where the algorithm starts in the parameter space, and ultimately where it will converge. If you happen to choose a batch of really bad starting values, then you’re likely to have way more local solutions than if somehow you knew where the global maximum was and chose a bunch of starting values close to that global maximum. In my thesis I chose a starting value method that appeared to not represent the parameter space very well, as my item response probabilities were always initialized to be near .50. Thus, all of my results, even the results from the linear model I described above, completely\nSo does that mean all of this is worthless? No! Now I am super curious to redo this investigation but with a different start value generation method. Maybe there will be a clearer, or even different, relationship between where the algorithm converges to and the final solution! Not to mention, even the tolerance critera set for convergence for the algorithm impacts the number of local solutions, so that’s another factor to consider.\nConclusion\nI said it once and I’ll say it again, this is very preliminary work, but I find it so interesting! I think it’s an underrated area of research and I really hope I can walk away from all of this with a set of guidelines or suggestions for applied researchers. Instead of throwing away the garbage (non- maximum likelihood solutions), maybe we can turn it into treasure (more insights about our model)!!!\nThanks for reading.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-18T15:49:41-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-08-05-startvalues/",
    "title": "Start value generation methods and their impact on Latent Class Models",
    "description": "Opening a can of worms...",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-08-05",
    "categories": [
      "research"
    ],
    "contents": "\nIntroduction\nParameter estimation is the process in which we obtain estimates for parameters that are part of our model of interest, which we then use to describe our population. Linear regression, structural equation modeling, and even just the mean, are examples where there are parameters that must be estimated. My area of research focuses on latent class models, which aim to identify subgroups in a population, where each of the groups differ on some specific latent variable. Each group (or class) has a set of parameters that describe it and differentiate it from another class. These parameters are item response probabilities and latent class prevalences. Item response probabilities tell us: what is the probability that an individual in class X will choose response option Y for item Z? Class prevalence tells us what proportion of the entire sample will be members of class X, as individuals can only be members of one class. We don’t know what the true item response probabilities or class prevalences are, so we must estimate them using the observed data we do have, which are individual’s responses to items.\nLatent class models are part of the broader family of mixture models, where individuals can be part of a group that has its own characteristics and parameters. These models are notoriously difficult to estimate. While there is more than one framework/perspective to estimation, what I’m talking about today pertains to Maximum Likelihood Estimation (MLE). With MLE, the goal of estimation is to pick values that maximize the probability of observing our data within the context of our model. Maximizing the probability involves choosing parameter values that maximize some function related to our model. In this case our desired function is \\[\\begin{equation}\nL(\\boldsymbol{\\gamma}, \\boldsymbol{\\rho} \\mid \\mathbf{y})\n= \\prod_{i=1}^{N}\n\\sum_{c=1}^{C} \\gamma_c\n\\prod_{j=1}^{J}\n\\prod_{r_j=1}^{R_j}\n\\rho_{j,r_j \\mid c}^{I(y_{ij} = r_j)}.\n\\end{equation}\\] I’m not going to explain this function in detail but this is what we’re working with. On a more abstract level- I like to explain estimation as hill-climbing. Our goal is, within a large and vast stretch of land, walk around until we find the hill with the highest elevation, and record our location once we reach it. In other words, we want to explore the entire parameter space and stop at the parameter values (location!) that correspond to the highest value of our function (elevation!). If there’s one giant hill and the rest is flat land, we have a pretty high chance of locating that hill, but if there are a bunch of hills, our chances of stopping on a hill that is not actually the highest, but maybe the second or third highest, is much higher. This is what is known in estimation as converging to a local optima/maxima. A local optima is the location of the solution with the highest value in a particular region, but not necessarily the highest value in the entire parameter space. We want the global maximum solution, but oftentimes what our software returns to us is a local maximum. There is no way to tell if we found a local or a global maximum unless you search every corner and crevice of the entire parameter space. In more than 2 dimensions I can’t even visualize what the parameter space looks like, but it would be quite complex and hard to do.\nThis is why starting values are important. If our parmeter space is super bumpy, rough, and complicated, where you start exploring is highly influential on where you’re going to end up. If you only start exploring from one little corner of the parameter space, you’re very likely to end up missing out on the true global maximum if it is all the way in the opposite corner. If we start exploring many, many, times (say…1,000+ times), each time starting in a different part of the parameter space.. the highest hill we reach (or in other words the maximum we reach), has a better chance of being the global maximum!\nThe importance of initializing the algorithm with many starting values has been written many times in best practice guides, but there is far less research about how exactly to generate those starting values. Expectation-Maximization (EM) algorithm is the most common estimation technique for latent class models, and it guarantees convergence to a local optima, but not necessarily a global optima. Starting values are highly influential for where the algorithm will converge in mixture models. Therefore, there is a recommendation to use many different starting values so that we have a higher chance of finding the global optima rather than settling for a local optima. However, we should think about quality > quantity. Not all starting value generation methods were created equal. A study done by Shireman et al. found that using k-means to find start values resulted in less local optima, while using a constrained EM technique found the best solution more often than other start value methods in most conditions. From what I know, there hasn’t been any published research that compares start value methods in the context of latent class models. I would love to write a paper on this soon, but for now I’m just going to start small. In this article I will compare a couple of start value methods and what solutions they converged to for a single simulated dataset.\nDifferent Start Value Methods\nRandom probabilities: This is probably the easiest method. Every parameter in our model is randomly generated from a uniform distribution, meaning that each parameter has equal probability of taking on any values between 0 and 1. This is the method used in the poLCA package in R.\nRandom Assignment: Each individual in the sample is randomly assigned to a class. Once all individuals have been assigned to a class, an initial set of item response probabilities are calculated based on who has been assigned to a class. Say hypothetically there are 100 individuals in our data, and all the items we’re looking at are binary (0 or 1 responses). 20 of these people have been randomly assigned to class 1. For each item, we will calculate item response probabilities for this class 1 by taking the average of everyone’s responses who are in class 1. Let’s say from these 20 people, the mean for item 1 is .7. That will be used for the starting value for item 1 class 1. We do this across all items and across all classes to get all of our starting values! This is actually the method I used for my master’s thesis simulation study.\nLatin Hypercube Sampling?! The name makes the method sound very futuristic and advanced. It is supposedly a more efficient alternative to grid search. Say I have 2 parameters X and Y that both range from 0 to 1. If I want to take 10 random samples from the entire parameter space, I can divide both X and Y into 10 equal chunks (chunk 1: 0-0.1, chunk 2: 0.1-0.2, etc). Sample randomly from each chunk so that we will have a vector of 10 values for X and a vector of 10 values for Y. Shuffle the two vectors so that we have 10 pairs of (X,Y) starting values. This method supposedly covers the parameter space well and is less computationally intensive than grid search. For grid search we would have every single X,Y combination such as (0.01,0.01), (0.01,0.02), (0.01.0.03)…as starting values to use for our algorithm. With 2 parameters this could be done, but with 10 or even 44 parameters (like in the example I will show), this will blow up quickly.\nThere are many other start value methods! K-means clustering has been suggested as a way to initialize starting values for mixture models, although this is less applicable for latent class model with categorical indicators. In that same article, iteratively constrained EM has also been suggested, where a few iterations of EM are allowed to run for a few steps, before being used as starting values for iterations that are allowed to iterate until they converge. In my personal uneducated opinion this seems a little bit redundant and doesn’t seem to address the issue of adequately exploring the parameter space.\nWhat I did\nJust as a simple thought experiment, I used a dataset that I generated for my master’s thesis simulation study. It has a sample of size 400, and the true parameter values that I used to generate responses are shown below.\n\n# A tibble: 11 × 5\n   TrueParameters   Class1 Class2 Class3 Class4\n   <chr>             <dbl>  <dbl>  <dbl>  <dbl>\n 1 Class Prevalence   0.7    0.15   0.1    0.05\n 2 Item 1             0.65   0.65   0.35   0.35\n 3 Item 2             0.65   0.65   0.35   0.35\n 4 Item 3             0.65   0.65   0.35   0.35\n 5 Item 4             0.65   0.65   0.35   0.35\n 6 Item 5             0.65   0.65   0.35   0.35\n 7 Item 6             0.65   0.65   0.65   0.35\n 8 Item 7             0.65   0.65   0.65   0.35\n 9 Item 8             0.65   0.65   0.65   0.35\n10 Item 9             0.65   0.65   0.65   0.35\n11 Item 10            0.65   0.65   0.65   0.35\n\nThere are 4 classes total, with class 1 being the largest as 70% of the sample belong to that group. With a small sample size, unbalanced class sizes, and high measurement error (an item response probability of .65 means that people have a .35 chance of saying the opposite response), it is highly likely that this model will be hard to estimate. Using this data, I fit latent class models using 4 different start value methods, and did 1,000 iterations for each method. I used the poLCA package in R with my own modifications to the start values as the structure to collect and store results is super simple to me. poLCA uses EM as the estimation algorithm.\nUltimately, I wanted to see if all start value methods lead to the same overall maximum likelihood solution, and if there was a difference in the number of local solutions found.\nWhat I found\n\n\n\nBefore I show the maximum liklihood value or the number of local solutions, I wanted to show the spread of starting values that each parameter took on across the 4 start value methods. Since I repeated each start value generation method 1,000 times, there is variability in that actual start values. Our goal is to see an equal spread of values across the 0 to 1 range in order to ensure that all areas of the parameter are explored.\n\n\n\nThe graph above shows the start value spread for all the parameters using random probabilities. Only the item response probability values are randomly generated, and the class prevalences starting values (V1-V4 in the graph) are always set to be 1/C, where C is the number of total classes. In this example where there are 4 classes each starting value for the latent classes is .25. For the item response probabilities you can see a decent spread from 0-1, but there are more values centered around .5 compared to 0 or 1. This method is the default for poLCA.\n\n\n\nThis graph also uses random probabilities (sampled from an uniform distribution) but instead of the class prevalences being fixed, they are sampled from a dirichlet distribution. I chose to sample from dirichlet because the class prevalences need to add up to 1, so sampling from a dirichlet distribution can account for that. You can see that for parameters V1-V4, there is now a spread of probability values. The distribution of the item response probabilities is the same as above.\n\n\n\nThe graph above shows the spread of parameter values for latin hypercube sampling. Latent class prevalences are also sampled from a dirichlet distribution. The distribution of item response probability start values looks visually different compared to the previous start value methods. There are more starting values closer to 0 and 1 compared to the previous methods, and it resembles more of a uniform distribution.\n\n\n\nThe last method is random assignment. This spread of starting values is the least uniform. There is a high concentration of starting values around the .4-.5 range. There is virtually no starting values close to 0 or 1. This is a little concerning because this is the start value method I used for my master’s work. I had no idea it would be this concentrated in the middle!\nLet us not panic yet…while the distribution of the start values may be different, let’s see how they impact the results. Turns out, all 4 methods found the same maximum LL value of -2658.298. If you are a regular researcher, that means that regardless of what start value method you used, you’ll reach the same solution. That’s good. Since I’m a methods researcher, I care a little bit more than just if the same maximum solution is reached. Are they reached the same number of times? What about the solutions that didn’t reach the maximum?\n\n\nTable 1: Frequency of Maximum LL Solution Between Start Value Methods\n\n\nStart Value Method\n\n\nFrequency of Maximum LL Solution\n\n\nLatin Hypercube Sampling\n\n\n110\n\n\nRandom Probabilities\n\n\n112\n\n\nRandom Probabilities w/ Random Prevalences\n\n\n89\n\n\nRandom Assignment\n\n\n101\n\n\nThe table above shows the frequency of the maximum LL solution for the various start value methods. While they are reached at relatively similar rates (remember that this is out of 1,000 iterations), there are slight differences. The random probabilities with random prevalences method results in the least amount of iterations reaching the maximum.\n\n\nTable 2: Comparing Local Solutions Found Between Start Value Methods\n\n\nStart Value Method\n\n\nNumber of Local Solutions Found\n\n\nLatin Hypercube Sampling\n\n\n63\n\n\nRandom Probabilities\n\n\n34\n\n\nRandom Probabilities w/ Random Prevalences\n\n\n100\n\n\nRandom Assignment\n\n\n16\n\n\nLastly, I want to take a look at the number of local solutions found. This has more variation compared to the frequency of maximum LL solutions. Random Probabilities w/ random prevalences converged to the most local solutions at 100, while random assignment only found 16 local solution. This is quite a big difference. Perhaps having random prevalences, with the possibility of really big or really small classes, allows for exploration of different parts of the parameter space that the random assignment method cannot do. My analysis will end here for today but there is promising future work to be done. Now that we observe differences in the number of local solutions, it would be interesting to look at the parameter estimates of these local solutions, or use the number of local solutions as a predictor for bias or class enumeration (deciding the optimal number of latent classes that fit the data).\nConclusion\nAs the sub-title of my article suggests, I really feel like I opened a can of worms in terms of my own research. My master’s thesis work is a simulation study comparing convergence behavior between EM and another estimation algorithm called Newton Raphson, and one metric that I use to compare the two are the frequency of local solutions each algorithm finds given the same set of starting values. Well in that work I arbitrarily chose random assignment as the start value generation method without giving it much thought. However, the results here suggest that the start value generation method chosen influences the number of local solutions…so my simulation results are only relevant in the context of the start value method. In my simulation study I did not find the number of local solutions to be a significant predictor of bias (of the final solution), given other simulation factors, but these results might change if I used a different start value method. The method I chose yielded the least amount of local solutions (at least based on this one dataset), while other methods yielded many more! I need to think about the potential implications of this.\nThis is super interesting to me and now I want to run my simulation results again but with a different start value method… but where is the time?! If there’s one thing to take away from this, is that take start value generation seriously!! Whatever software you’re using to estimate models like latent class models, do some digging to see what method they use.\nThanks for reading. Take care.\n\n\n\n",
    "preview": "posts/2025-08-05-startvalues/randompr.png",
    "last_modified": "2025-08-06T20:43:38-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-07-28-discrete-continous-LV/",
    "title": "Discrete vs Continous Latent Variables",
    "description": "\"All models are wrong, but some are useful\" - George Box",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-07-28",
    "categories": [
      "research"
    ],
    "contents": "\nIntroduction\nLatent variables are constructs (things/concepts) that we can not observe directly, but we believe influence our observed data that we collect. Depression is a latent variable that we cannot assess directly, but we hope that a questionnaire like PHQ-9 or Beck’s Depression Inventory are proxies that are able to effectively capture depression. The items that we use are assumed to have measurement error because we cannot 100% perfectly capture the latent variable (otherwise it wouldn’t be latent anymore..).\nBecause we can’t directly see the latent variable, there are different forms that it can take on. Depending on the theorized form of the latent variable, researchers will typically work with a subset of models in hopes of distinguishing individuals on this latent variable. The latent variable can be thought of as continuous, taking on a quantity that can be used to rank order and quantitatively compare individuals against one another. Factor analysis or Item Response Theory (IRT) are models that assume the latent variable is continuous, and both aim to describe the underlying structure of a set of observed variables given the latent variable.\nOn the other hand, discrete latent variable models like Latent Class Analysis (LCA) or Latent Profile Analysis (LPA) assume that individuals are divided into discrete subgroups based on the latent variable, with qualitative differences between groups rather than quantitative. We wouldn’t say that individuals in one group score higher on the latent variable compared to another. Latent class analysis has been used in PTSD, depression, and eating disorder research to uncover groups where individuals exhbit different patterns of behavior from one another, but are not theorized to be less or more severe than other groups.\nMethods that assume a discrete latent variable or a continuous latent variable can both be used to investigate heterogeneity in a sample and to characterize individuals. The question I’ve been grappling with is, how do you know which type of model to use? There is no test or method to determine whether your latent variable is discrete or continuous. Rather, the form of the latent variable is assumed from theory or previous research, and the choice of model will follow. While most researchers just conduct analysis assuming one framework, I’m curious to see what I can learn if I use both continous and discrete latent variable methods on the same dataset. What kind of conclusions can I make, and do conclusions from continuous vs discrete latent variable models contradict or overlap with one another?\nAn area that I’ve been interested in during my time in graduate school is gerontology and healthy aging. One component that is essential for healthy aging is independent physical functioning. Activities of Daily Living (ADL) are a set of items developed by Katz in 1950 that are used to assess which basic activities, like getting dressed or bathing, that adults are able to complete without assistance. ADLs have been used to identify elders with severe mobility issues, and have also been used as predictors of mental wellbeing and mortality. Instrumental Activities of Daily Living (IADLs) are similar to ADLs but encompass more complex activities like taking public transportation or managing finances, and may be measures of cognitive functioning on top of physical functioning. There have been both discrete latent variable methods, as well as continous variable methods used to assess functional ability, with results supporting both models. My goal is to take a stab at both different types of methods myself and write up summaries so I can compare and contrast the two. For continuous latent variable methods I will use both EFA and IRT, and for discrete variable I will use LCA.\nThe dataset that I will be using to carry out my latent variable investigation is the Chinese Longitudinal Healthy Longevity Survey, a national study that assesses quality of life and health status of adults 65 years and older across 22 provinces in China. The goal of the study is to evaluate which biological, social, and environmental factors contribute to longevity and successful aging, and currently has 9 waves spanning from 1998 to 2021. I am interested in indicators related to functional anility. There are 6 items related to ADL: bathing, dressing, toilet, transfer, continence, and feeding. Additionally, there are 8 IADL items: visiting neighbors, shopping, cooking, doing laundry, walking 1km, lift a 5kg weight, crouching/standing, and taking public transportation. For these 14 items participants can respond as either being able to complete the activity independently, able to complete with assistance, or unable to complete. For the purposes of the analysis, ADL and IADL items will be dichotomized so that those who require any assistance will be grouped into one category. I will be using data from the 2005 wave of the study, and for the sake of analysis I will only include elders who have complete data on all of the ADL/IADL items, for a total sample size of 2,637.\nExploratory Factor Analysis (EFA)\n\n\n\nThe goal of EFA is to find the underlying factor structure that explains the correlation between observed items in a dataset. The factor loadings give information about the strength of the relationship between the factor and the item. Based on these factor loadings we can estimate individuals’ factor scores, tell us where they fall along the latent variable continuum. The factor scores can be used in regression or other methods compare various groups on this latent trait.\nTo begin, we must construct a correlation matrix of the items in our dataset. Because these items are all dichotomous, a tetrachoric correlation needs to be estimated before doing EFA. It estimates the Pearson correlation assuming that the binary items are actually coarse approximations of normally distributed latent variable.\n\nCall: tetrachoric(x = dt)\ntetrachoric correlation \n    E1   E2   E3   E4   E5   E6   E7   E8   E9   E10 \nE1  1.00                                             \nE2  0.85 1.00                                        \nE3  0.89 0.94 1.00                                   \nE4  0.87 0.91 0.95 1.00                              \nE5  0.53 0.48 0.61 0.60 1.00                         \nE6  0.85 0.93 0.93 0.86 0.67 1.00                    \nE7  0.77 0.85 0.86 0.83 0.58 0.85 1.00               \nE8  0.77 0.83 0.81 0.83 0.41 0.79 0.90 1.00          \nE9  0.77 0.84 0.81 0.83 0.47 0.79 0.87 0.85 1.00     \nE10 0.74 0.80 0.82 0.78 0.53 0.78 0.80 0.82 0.93 1.00\nE11 0.69 0.78 0.76 0.76 0.37 0.70 0.82 0.85 0.78 0.77\nE12 0.64 0.65 0.76 0.75 0.37 0.62 0.76 0.78 0.74 0.77\nE13 0.63 0.63 0.65 0.70 0.35 0.56 0.70 0.76 0.70 0.71\nE14 0.67 0.74 0.72 0.71 0.24 0.65 0.82 0.84 0.75 0.72\n    E11  E12  E13  E14 \nE11 1.00               \nE12 0.86 1.00          \nE13 0.83 0.86 1.00     \nE14 0.80 0.80 0.81 1.00\n\n with tau of \n   E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11 \n-1.88 -2.35 -2.41 -2.43 -2.53 -2.58 -1.89 -1.39 -1.40 -1.30 -0.97 \n  E12   E13   E14 \n-0.98 -0.76 -0.77 \n\nAfter checking that the ADL/IADL were well correlated with one another, we used common factor analysis with weighted least squares estimation (due to prescence of categorical items). In order to determine the number of factors to estimate I used parallel analysis, which simulates a random dataset with the same number of variables as the existing dataset and extracts eigenvalues. The number of eigenvalues of the existing dataset that are larger than the eigenvalues extracted from the simulated dataset is the number of factors to be estimated.\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \nCall: fa.parallel(x = rr$rho, n.obs = nrow(dt), fa = \"fa\")\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n Eigen Values of \n\n eigen values of factors\n [1] 10.54  0.82  0.18  0.14  0.08 -0.02 -0.05 -0.06 -0.07 -0.12 -0.16\n[12] -0.20 -0.21 -0.32\n\n eigen values of simulated factors\n [1]  0.41  0.10  0.08  0.07  0.05  0.03  0.01  0.00 -0.02 -0.03 -0.05\n[12] -0.06 -0.08 -0.11\n\n eigen values of components \n [1] 10.76  1.18  0.57  0.37  0.29  0.21  0.17  0.14  0.13  0.09  0.06\n[12]  0.03  0.00  0.00\n\n eigen values of simulated components\n[1] NA\n\nBased on parallel analysis I fit a EFA model with 5 factors, along with oblim rotation, a method that allows factors to be correlated with one another and loadings with a more ideal structure. By ideal structure, I mean items that have high factor loadings on only one factor and low loadings on other, allowing for easier interpretation of the factors. For example, say that I fit 2 factors to my set of 14 ADL/IADL items. Perhaps all of my ADL items (feeding, dressing, continence, etc) have high factor loadings on factor 1 and small factor loadings on factor 2. Additionally, my IADL items do the opposite: they have high factor loadings on factor 2 rather than factor 1. This would make interpretation of the factors easier, as we could say that factor 1 represents basic or essential functioning while factor 2 represents more advanced physical functioning. If the continence item had a high factor loading on both factor 1 and 2, it would be less clear on how we would interpret and distinguish these two factors.\n\n\nLoadings:\n    WLS1   WLS2   WLS3   WLS4   WLS5  \nE1   0.813  0.131                     \nE2   0.883  0.115                     \nE3   0.872  0.136                     \nE4   0.871  0.117                     \nE5   0.450  0.473                     \nE6   0.808  0.205                     \nE7   0.881                            \nE8   0.900                            \nE9   0.872                            \nE10  0.856                            \nE11  0.881 -0.136                     \nE12  0.856 -0.166                     \nE13  0.821 -0.193                     \nE14  0.855 -0.194                     \n\n                WLS1  WLS2  WLS3  WLS4  WLS5\nSS loadings    9.808 0.456 0.000 0.000 0.000\nProportion Var 0.701 0.033 0.000 0.000 0.000\nCumulative Var 0.701 0.733 0.733 0.733 0.733\n\nWhile the results from parallel analysis suggest 5 factors, looking at the loadings and proportion of variance explained tells a different story. Factors 2-5 do not have any items with high factor loadings, and the amount of variance among the items that they explain is incredibly low compared to factor 1. Based on these results I decided to restimate the model with only 1 factor instead of 5.\n\n\nLoadings:\n    WLS1 \nE1  0.844\nE2  0.910\nE3  0.904\nE4  0.898\nE5  0.562\nE6  0.856\nE7  0.890\nE8  0.885\nE9  0.873\nE10 0.860\nE11 0.848\nE12 0.816\nE13 0.775\nE14 0.809\n\n                WLS1\nSS loadings    9.929\nProportion Var 0.709\n\nThe results from an EFA model with only one factor estimated show that all 14 ADL/IADL items load onto a single factor. This supports one theory that ADL and IADL items are unidimensional based on this dataset.\nItem Response Theory (IRT)\nThe goal of IRT is to model participants’ probability of responding to a set of items as a function of a hypothesized latent trait. Similar to EFA, we are able to estimate each participant’s latent trait value along a continum and use that value for further comparison and analysis. A core characteristic of IRT is that the model also estimates item parameters that describe the item regardless of the sample used. The parameters depend on the specific IRT model chosen, but could include item difficulty, item discrimination, and guessing parameters. Item difficulty refers to the latent trait needed to have a 50% chance of answering the item correctly (or in non-education settings, the probability of endorsing the item). Items that require a higher latent trait value in order to have that 50% chance have higher difficulty. Item discrimination is a quantity that refers to how well the item differentiates people with similar latent trait values. It can be thought of as a slope. The steeper the slope is, or the larger the item discrimination value is, there’s a higher chance of endorsing the item with every small increment of the latent trait. Guessing parameters are more common in education settings where students may have a chance of randomly selecting the correct answer (like in multiple choice tests) rather than actually being proficient in the latent trait, so this parameter accounts for that possibility.\nIRT is an incredibly useful tool to not only assess the individuals in the sample, but to learn about the characteristics and quality of the items. It is desirable to have items that range in difficulty and have high discrimination so that the survey or assessment overall can assess people with high, low, and inbetween values on the latent continum. This is random but there’s a quote in a fundamental IRT book that I really like: “Researchers should spend more time investigating their scales than investigating with their scales.” Using IRT models to evaluate measures is super important because establishing validity is absolutely essential before interpreting results. Oftentimes researchers are eager to use their tools right away without stopping to check if their tools are sharp enough!\nBecause we have some earlier knowledge from fitting the EFA model that these set of ADL/IADL items may be unidimensional, we will fit an IRT model that assumes unidimensionality. There is a 1PL model which only estimates difficulty parameters, which is often the first model that is fit. After fitting the 1PL model I fit a 2PL model which also estimates discrimination parameters. Since this 2PL model was a significantly better fit to the data, I will only discuss the results of the 2PL model. Overall, the fit of the 2PL on this set of 14 ADL/IADL items was generally satisfactory after evaluating some fit statistics. The Root Mean Squared Error of Approximation (RMSEA) was .04, which is below the .05 threshold of adequate fit commonly used in research. In addition, the Comparative Fit Index (CFI) was .99, above the well-known criteria of .95. These fit statistics never guarantee fit or that the best model is chosen, but provide evidence to support its continued use and interpretation of its parameters.\n\n           a          b g u\nE1  2.682206 -2.2727847 0 1\nE2  4.086769 -2.6274767 0 1\nE3  4.901545 -2.6476169 0 1\nE4  4.485009 -2.7009171 0 1\nE5  1.394256 -4.4036514 0 1\nE6  3.649285 -3.0013932 0 1\nE7  4.019452 -2.0609180 0 1\nE8  4.417876 -1.4785459 0 1\nE9  3.896057 -1.5269327 0 1\nE10 3.517278 -1.4424198 0 1\nE11 4.328125 -1.0458542 0 1\nE12 3.869158 -1.0694429 0 1\nE13 3.453457 -0.8526636 0 1\nE14 3.305270 -0.8660019 0 1\n\nThe table above shows the discrimination (a) and difficulty (b) estimates for this dataset. The least difficult item is continence, whether one is able to use the restroom without assistance. One does not need to be on the high end of this latent trait spectrum, in this case the latent trait being functional ability, in order to have a 50% chance of being able to independently do this item. This estimate tracks with general gerontology research which shows that loss of continence is usually the last functional limitation that occurs and usually signals severe health decline. Other low difficulty items are feeding and getting dressed. On the other hand, the most difficult items are being able to crouch and stand 3 times as well as taking public transportation. Individuals need to have higher functional ability in order to have a higher probability of being able to do this activity.\n\n\n\nThe figure above shows information curves for each of the 14 items. These information curves let us know how informative these items are at different values of the latent trait, represented by theta. The higher the peak, the more information this item provides for people who are at that theta level. Both the item discrimination and the item difficulty are used to construct these information curves, and one can see that different items provide information for different values of the latent trait. For example, E5 (continence) provides the most information for people who are low on functional ability. This makes sense intuitively if you think about the fact that people with higher functional ability are all able to use the bathroom independently, so this item is not able to distinguish people who are past a certain threshold of basic functional ability. On the other hand, E14 (using public transportation) provides the most information for people who have higher functional ability. People with low functional ability who struggle to use the bathroom or get dressed are all going to be not able to use public transportation, so this item is not very helpful at differentiating people with a lower range of functional ability.\n\n   item   S_X2 df.S_X2 RMSEA.S_X2 p.S_X2\n1    E1  7.737       8      0.000  0.460\n2    E2  1.258       2      0.000  0.533\n3    E3  6.709       2      0.030  0.035\n4    E4  1.984       2      0.000  0.371\n5    E5  5.710       7      0.000  0.574\n6    E6  2.807       1      0.026  0.094\n7    E7  7.342       6      0.009  0.290\n8    E8 19.749       6      0.029  0.003\n9    E9  4.483       6      0.000  0.612\n10  E10  7.652       6      0.010  0.265\n11  E11  4.594       4      0.008  0.332\n12  E12  4.776       5      0.000  0.444\n13  E13 21.265       5      0.035  0.001\n14  E14 14.664       5      0.027  0.012\n\nWhile the overall model of fit is adequate, it is also possible to run item-level fit statistics to identify items that are not performing as expected based on the model and that could be reviewed. Large S-\\(\\chi^2\\) values with significant values indicate items that deviate significantly from the 2PL model. In this example we have two poor-fitting items using a .05 threshold: Toileting and shopping independently. If one was using IRT for scale development and to add/remove items, it is recommended to review these item fit statistics and do further analysis to investigate whether these two items should be excluded or not. Since my focus here is not scale development I will not proceed with any further adjustments.\n\n\n\nLastly, I want to turn our focus on the person parameters. Based on the estimated item parameter values, we can also estimate latent trait scores for each person in the dataset. The graph above shows the distribution of theta scores for this sample of older Chinese adults. Overall, most adults in the sample have relatively higher functional ability, while there are a smaller subset that vary in terms of severity of functional disability. The actual numerical value of the theta scores should not be interpreted, but can be used to compare comparisons within the sample. This distribution aligns with the sample characteristics, as these older adults have been sampled from the general population and excludes institutionalized older adults who may have severe functional limitations.\nOverall, using IRT on this data shed some light about which items are informative for describing individuals that are located on different parts of the functional ability variable. While we obtain more information about our items compared to EFA, this is more of a secondary step after EFA rather than a competing model during step 1 of data exploration. In order to fit the 1PL and 2PL IRT models, the assumption of unidimensionality must be justified. Results from our EFA where we allowed for the possibility of multiple factors to explain the correlation of our items indicated that one factor was sufficient for explaining our data, so thus I decided to fit these IRT models. The distribution of latent trait scores from both EFA and the 2PL model were very similar. My analysis here stops short but a next step I could take is to use these latent trait scores as a predictor for some type of outcome. In the context of aging it could be a useful predictor of depression and mental wellbeing (although the direction of this model is kind of undetermined because mental wellbeing could also predict functional ability too..). I don’t do gerontology research or have any strong hypotheses so I will just stop here and move on to the discrete variable case: latent class models!\nLatent Class Analysis (LCA)\nFirst introduced by Paul Lazarsfeld in 1950, latent class analysis (LCA) is a statistical method used to relate a set of observed indicators to a latent variable by identifying underlying and unobserved subgroups within a population. The latent variable is discrete and consists of 2 or more mutually-exclusive subgroups, also known as classes, that individuals belong to. Individuals in each class are assumed to be homogeneous and have the same item response probabilities, which is the probability of endorsing a particular response for an item given class membership. In addition, the model also includes latent class prevalences, which represent the proportion of individuals in the sample that are members of a class.\nIn order to select the number of classes that best describe the sample, several fit statistics as well as examining the interpretability of the solution can be considered. In this analysis I decided to use AIC and BIC, which are fit statistics that are calculated based on considering the likelihood value of the model and the number of parameters and/or sample size. Based on these, the 5 class model fit the data the best, and the solution is shown below.\n\n\n\n\n\n\nThere are 5 different bar graphs, each indicating the estimated parameter values for a specific class. The 14 bars represent the item response probabilities given membership in that class. The decimal value above each graph is the latent class prevalence: the proportion of individuals in the sample that are estimated to belong to this class. The largest class is the class in the bottom middle, with 75% of the sample being estimated to be part of this class. Individuals in this class have a high probability of being able to independently do each of the activities. We could label this class “no functional limitations”. The next class on the left with a prevalence of 15% captures individuals who are able to do all ADL activities, but struggle with more physically intensive IADL activities of walking 1 kilometre, carrying 5kg, crouching, and taking public transportation. The class on the top right with a prevalence of 5% describes individuals who are able to do ADLs but no IADLs other than visiting neighbors. The top middle class with a prevalence of 3% describes individuals who can do all ADLs and IADLs besides making food and washing clothes. Lastly, the smallest class with only 1% prevalence describe individuals who only have continence (the ability to use the bathroom independently).\nThese classes tell an interesting story about potential different groups of Chinese elders, but raise questions as well. Normally, these estimated classes can be used in further analysis to determine what factors increase likelihood of being a member of each of the classes. I would hypothesize that males are more likely to be in the latent class with difficulties making food or washing clothes compared to females. This might not be because males actually struggle more, but that because of societal structures they typically did not do those tasks in the first place, not because of their functional ability. This hypothesis warrants further investigating. I would also hypothesize that older adults (90+) are more likely to be in the class with the most severe limitations compared to younger older adults. Further analysis is needed, but the final solution illustrated here shows the heterogeneity in functional ability in this sample of almost 3,000 adults.\nConclusion\nThis was a great excercise for me to practice going through all of the steps of fitting each of these three models and writing up some takeaways from the models, but I’m still not 100% satisfied. It was interesting to see agreement between IRT and EFA that these ADL/IADL items are unidimensional, and that estimated latent trait abilities were quite similar as well. The IRT model was a lot more informative in identifying which items were difficult and better able to distinguish individuals based on functional ability. On the other, the classes from the latent class model do not support unidimensionality at all. Rather than having 5 classes where each one has a few less items that individuals can independently do, these 5 classes represent 5 qualitatively distinct subgroups that differ on the combination of ADL/IADLs they can perform independently. Three classes were different combinations of IADLs that individuals could complete along with one class with severe functional limitation and one class with no functional limitation.\nThe only overlap that I could see between these two methods is that a dominant group of individuals with no functional limitations was identified in both continous and discrete latent variable methods. The other groups produced by the latent class model are not identified in the continuous latent variable models. Ultimately, this exercise reinforced how different modeling approaches can yield different insights depending on the assumptions they make about the underlying structure of the data. It would be interesting to examine hybrid discrete/continuous latent variable models such as factor mixture models and what kind of story it can tell.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2025-07-28-discrete-continous-LV/2025-07-28-discrete-continous_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-08-05T18:57:54-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-07-19-poLCA/",
    "title": "poLCA: Starting values for Class Prevalence Probabilities",
    "description": "Sometimes maybe less is more..",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-07-19",
    "categories": [
      "research"
    ],
    "contents": "\nIntroduction\nThis post assumes familiarity with latent class models. [Here] is an intro if you aren’t.\nWhen estimating latent class models, starting values are needed to get the estimation algorithm going. When I first learned about latent class models, I came across this R package poLCA which allows for latent class models to be estimated very easily. After 30 minutes of poking around I was able to use the package to estimate a simple latent class model. All of the parameter estimates and model fit information are easy to access and you can even plot the class solutions in a 3D bar graph using one of the functions in the package!\nIt wasn’t until a year or so later during graduate school where I became interested in estimation. Expectation-maximization (EM) algorithm is the most common algorithm for estimating latent class models, and is what is used in the poLCA package. After understanding the basic gist of the EM algorithm I was always curious about how it was actually implemented, but other software like STATA and MPLUS do not have publicly accessible code. That’s why I started looking into the code for poLCA which is open-source and able to be viewed directly on github. That’s when I noticed that while there was code to randomly generate item response probabilities from a uniform distribution or provide user input for item response probabilities, there wasn’t that option for the latent class prevalence probabilities. The default was that the starting latent class prevalence probabilities were evenly split by the number of classes estimated. For example, if 4 classes are estimated, then the starting value probabilities for each class is .25.\nMotivation\nThat got me thinking…would this default specification of equal latent class sizes for the starting value have an impact on estimation? Around this time I was starting to think about how estimation of latent class models could vary: what makes them easy or challenging to estimate? Conversations with my advisor helped me develop some hypotheses about challenging estimation conditions such as small sample sizes and items with high measurement error. Unbalanced latent class proportions came up as a factor worth examining, since smaller classes means that there are fewer observations in the dataset that will contribute information to the model when estimating parameters for that smaller class.\nHaving unbalanced latent class prevalences (such as 4 classes with prevalences of .75, .12,.08,.05) is common in alcohol research, where high or frequent substance use represents a smaller proportion of the population. In a simulation study examining the impacts of misspecification on class enumeration, the authors found that compared to data generated from a two class model with 50%/50% membership in the two classes, class enumeration indices (used for deciding the optimal number of classes to fit the data) performed worse when the two classes had a 80%/20% membership instead. This research shows support for the idea that unbalanced class sizes impact the estimation process and the final solution.\nNow returning back to the issue of starting values. Say we have data generated from a latent class with unbalanced class sizes, making it very likely to have a complicated likelihood surface (aka more challenging to find the best parameters that describe the data). If we start our estimation algorithm with starting values where the classes are equally sized, will the algorithm struggle to find the optimal parameter values? When estimation latent class models we most often follow the principles of maximum likelihood estimation. Given our observed data that we have (response patterns to a set of items), we choose the set of parameter values, item response probabilities and latent class prevalences, that maximize the likelihood of observing the data. I like to think of the likelihood as a vast landscape with lots of hills and valleys. We want to find the location of the highest peak, which represents the set of parameter values that correspond to the best likelihood, but we might get stuck at a lower peak and call it a day there instead. These estimation algorithms might stop at what is believed to be the highest peak, but it reality it is not. This is called reaching a local solution, rather than the maximum solution. So my question is, by starting our algorithm in a location that is a bit further from the true parameter values (aka further away from the highest peak), will the algorithm be more likely to land on a local solution? If that is the case, then maybe we could propose a way to allow for unbalanced class size estimates to be used as starting values rather than only equal class sizes.\nWhat I Did\nWhat I did was quite simple. I copied the main poLCA function from the package’s github and modified a small chunk of the code so that the user could specify whatever latent class prevalence values they wanted for the initialization of the algorithm, rather than only always using equal class sizes. Here’s what I wanted to test: if I gave the same dataset and item response probability starting values to two versions of poLCA, one where the latent class starting values are equal, and one where they are not equal, would they both reach the same solution?\nI only used one dataset in this example, and so more datasets should be done in order to have a definitive answer, but nonethless I think these initial results can be illuminating.\n\n\n\nThe graph above shows the parameters used to generate the dataset. These are the true values that we hope to recover from fitting our model. Each of the bars represents the probability of saying “no” to a particular item. The graph in the upper left hand corner shows the solution for class 1. .05 is the latent class prevalence, this means that 5% of our sample is estimated to be part of this class. Based at looking at the bars, in this class individuals have a 35% of saying “no” to all items X1-X10 in this simulated fake data example. If we translate this to a substance use example just for kicks, let’s say that items X1-10 ask whether someone has consumed different types of substances such as alcohol, marijuana, cocaine, etc., with one drug for each question. Because individuals in this class have a low probability of saying no to each of these items, we might label this class as “high substance use”. This class only represents 5% of our population, which could track with what we see in real world research with smaller populations of poly-substance users. The class on the bottom right would represent “low substance use”, as shown by high probability of saying “no” to all of the items X1-X10.\nThis class represents 70% of the entire sample, which is a large majority! The other two classes show varying response patterns and highlight the heterogeneity in responses.\nResults\n\n\n\nSo I ran two models. One model with equal latent prevalence probabilities (.25,.25,.25,.25) for starting values, and the other model with unequal latent prevalence probabilities (.7,.15,.1,.05). Both models had the exact same starting values for the item response probabilities. The results after estimation are shown in the figure below. Based on just looking at the top most graph on both the equal and unequal models, we can see some differences in the final solution. When using equal class probabilities as the starting value, we see that the chance of saying “no” for item X1 is 56%, while on the other hand when we use unequal class probabilities, the chance of saying “no” for item X1 is 37%. This difference in parameter values would change the interpretation of the class. The first solution suggests individuals in this class are likely to say “no” for item X1, while the second solution suggests the opposite! A similar situation arises for items X9 and X10 for the same class! Notice that these large differences between start value method only occurs in the smallest latent class. This goes back to what I mentioned earlier about smaller classes being harder to estimate due to limited information!\nWhich solution is better? Based on looking at the likelihood value, where we aim to choose the solution with the largest likelihood value, we would go with the solution produced by starting with equal latent class prevalence probabilities. Does this mean all of this was moot and that starting with equal probabilities is always the way to go? Not yet…\nWhen fitting latent class models, the issue of algorithms getting stuck on local solutions rather than the maximum is well known. Thus, textbooks and articles about best practices for latent class models frequently suggest running many iterations with varying starting values, and then choosing the solution with the largest likelihood value. So given that my earlier exploration was just one iteration, let’s ramp it up to 1000 iterations. For each method of starting values, equal and unequal latent class prevalence probabilities, we will run the model 1000 times (making sure that both methods have the same set of 1000 sets of item response probability starting values), and choose the best solution for each of the methods and compare. This way aligns more closely with how researchers would actually fit these models. It is possible that despite some differences in solutions, the overall best solution found by the two methods over 1,000 iterations might just be the same.\n\n\n\nLo and behold, the best solution found by both methods over 1,000 iterations yielded the same solution! There are some slight differences in the parameter estimates but they are beyond the thousandths decimal place so for practicality I will say that they’re the same solution. So that’s interesting! One important thing I wanted to highlight is that these solutions are pretty far away from the true data generating parameters! Despite the similarity in the final solution between the two methods, these results highlight the difficulties of estimating this model with this dataset. This dataset not only has item response probabilities with high measurement error, but only has a sample size of 400, which is smaller than the recomended number for fitting latent class models!\nWe’ve established in this example that the overall final solution doesn’t differ much, but I’m still curious about whether there are some differences iteration by iteration between the two methods. If so, is there one method that tends to find better solutions over the other? I made two tables that compare how many times one method was superior to the other when they both used the exact same set of item response probability starting values over the 1,000 iterations.\n\n\nTable 1: Comparing Convergence Between Start Value Methods\n\n\nDescription\n\n\nValue\n\n\n% of iterations where both fail to converge\n\n\n14\n\n\n% of iterations where equal class probabilities fail but unequal doesnt\n\n\n19\n\n\n% of iterations where unequal class probabilities fail but equal doesnt\n\n\n24\n\n\nBefore we can compare solutions, we should look at convergence first. Not all iterations will successfully converge. Failure to converge could be due to the location of the starting values causing the algorithm to get stuck in an area where it cannot improve. As shown in Table 1, we can see that using unequal latent class prevalence probabilites as starting values resulted in slightly higher convergence failures compared to using equal probabilities.\n\n\nTable 2: Comparing Solutions Between Start Value Methods\n\n\nDescription\n\n\nValue\n\n\n% of iterations where equal and unequal class probabilities have same solution\n\n\n44\n\n\n% of iterations where unequal class probabilites has better solution\n\n\n24\n\n\n% of iterations where equal class probabilites has better solution\n\n\n32\n\n\nTable 2 compares the solutions found by the two start values method. A better solution in this case is defined by the solution with the higher likelihood value. Almost half of the iterations resulted in the two methods converging to the same solution. When they didn’t, the equal probabilities starting values was slightly more likely to find a better solution.\nConclusion\nI don’t think my work in this area is done yet, but the results that I have found so far are interesting. Examining results on a granular level show some differences in solutions when using equal latent class prevalence probabilities as starting values for the algorithm versus using unequal latent class prevalence probabilities. However, the overall best solution out of 1,000 iterations produced by the two methods are the same. Further data conditions and simulations should be run before any strong conclusions are made, but from my perspective I believe the poLCA package should allow users to specify different latent class prevalence probabilities as starting values as an option. Even if it does not ensure a better likelihood solution is found, it could help ensure that the complicated likelihood surface is adequately explored in all of its nooks and crannies (in a high dimensional sense..).\nOverall this was super fun to explore and I’m glad I took the time to do a deep dive on an existing package’s code! My takeaway message I hope to convey is that estimation is extremely hard and we should thoroughly evaluate all the decisions we make during the estimation process, whether that is how we generate starting values or even which algorithm we choose!\nIf you’ve made it this far thanks for reading.\n\n\n\n",
    "preview": "posts/2025-07-19-poLCA/Rplot1.png",
    "last_modified": "2025-08-05T18:38:29-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-07-18-welcome-again/",
    "title": "My First Post (PhD verison)",
    "description": "It's been 4 years since my last post! I still don't know how to use git",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-07-18",
    "categories": [
      "food",
      "research"
    ],
    "contents": "\nHowdy! It’s been a while since I last tried to start up this blog again. I want to practice writing up and sharing some of my research I’ve been working on for the past 3 (!!!!) years. The last time I wrote here I was still a wee little undergraduate student who had 0 idea about grad school and alas… here we are.\nIt took me a long time to get to the research topics that I’m interested in now. I started my program not even knowing what latent class models are, so I’d like to think I worked myself from the ground up! Broadly, I am interested in digging deeper into measurement models that psychology researchers use. My own personal theory is that the ease of software allows researchers to run extremely complex models with just one line of code and return results in a matter of seconds, leading many to believe that the models are easy to compute and extract values. Most of the time these models are extremely challenging to estimate, and software programs makes a lot of choices that researchers don’t about, which can impact their results! One choice in particular is the estimation algorithm used to obtain parameter estimates. My current work, which I will detail in a separate post, compares how two different estimation algorithms result in different solutions for the same model and same dataset!\nThe model I’m focusing on at the moment is latent class models! Latent class models aim to characterize individuals in a sample into different groups, where each group differs on some unobserved (latent) variable. Latent class models have been used in substance research, education, and so much more. Pretty much every field I’ve encountered in research will have at least 1-2 papers using latent class models. Latent class models differ from machine learning methods that also aim to classify people into groups (k-means clustering) in that the items or predictors are assumed to have measurement error. We assume that the thing (or latent variable) that distinguishes different groups is unobservable, and that our items are just an approximation of the real thing. Examples include depression, alcohol dependence, burnout, and disordered eating.\nMore specifically, here are some of the topics I’m interested in and what I hope to write articles on…\nDo Newton-Raphson and Expectation-Maximization Algorithms traverse the latent class likelihood surface in the same way?\nDo different types of parameter restrictions (equality, inequality, bounds) impact estimation of latent class models in different ways?\nSome less fleshed out research ideas include..\nSimulated annealing + genetic algorithms and how they compare to traditional algorithms\nstart value quality\nMy first article will be how starting values are generated for a popular R package poLCA. Please stay tuned.\nOf course it wouldn’t be me if I didn’t talk about food..I wanted to highlight a new spot in Seattle I really like called Many Bennies. They serve New Zealand style ice cream where they blend your choice of fruits with a sweet cream base! It tastes like no other ice cream that I’ve ever had before. It has the lightness of frozen yogurt but it’s creamy and not tart at all! After eating this ice cream I realize how insanely sweet traditional ice cream places are…I’m not even a huge fruit fan but after trying this place I can never go back to Ben and Jerrys or Molly Moons haha. I got blueberries and strawberries and it was the perfect combination. They also give free lactaid which is super cool!\n\n\n\nIf you made it this far, thanks for reading. Take care!\nJessica\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-05T18:38:29-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-my-first-post/",
    "title": "My First Post",
    "description": "I'm really struggling with the distill package! And git commands!",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nI spent about 1 hour deleting and cloning and merging different repositories.. instead of properly learning git commands I’ve been relying on the integrated git feature in RStudio.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-05T18:38:29-07:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Crouton Thoughts",
    "description": "The world needs to hear what I have to say!",
    "author": [
      {
        "name": "Jessica",
        "url": "https://github.com/jeyu22"
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\n\n\n\nHi! My name is Jessica. I’m a senior at Amherst College studying statistics and psychology. I made this blog for fun in hopes of motivating myself to be a more active programmer and data scientist! I love R and #TidyTuesday. Besides statistics I enjoy finding new places to eat in the D.C/Maryland/Virginia area.\nHere’s a list (mainly for myself) of posts I want to make in the next few months:\nRatings of all the restaurants/cafes I’ve tried (ideally on a leaflet plot…)\nFood reviews…\nTidy Tuesday challenges\nSide projects..\nI’m also writing a statistics thesis this upcoming fall, so hopefully I’ll post updates as that progresses!\nWhy croutons?\nThey’re versatile: as a topping for salad, a tasty snack, or an entire meal. My favorite croutons are the ones that come with the caesar salad at Costco.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-05T18:38:29-07:00",
    "input_file": {}
  }
]
