[
  {
    "path": "posts/2025-07-28-discrete-continous-LV/",
    "title": "Discrete vs Continous Latent Variables",
    "description": "\"All models are wrong, but some are useful\" - George Box",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-07-28",
    "categories": [
      "research"
    ],
    "contents": "\nIntroduction\nLatent variables are constructs (things/concepts) that we can not observe directly, but we believe influence our observed data that we collect. Depression is a latent variable that we cannot assess directly, but we hope that a questionnaire like PHQ-9 or Beck’s Depression Inventory are proxies that are able to effectively capture depression. The items that we use are assumed to have measurement error because we cannot 100% perfectly capture the latent variable (otherwise it wouldn’t be latent anymore..).\nBecause we can’t directly see the latent variable, there are different forms that it can take on. Depending on the theorized form of the latent variable, researchers will typically work with a subset of models in hopes of distinguishing individuals on this latent variable. The latent variable can be thought of as continuous, taking on a quantity that can be used to rank order and quantitatively compare individuals against one another. Factor analysis or Item Response Theory (IRT) are models that assume the latent variable is continuous, and both aim to describe the underlying structure of a set of observed variables given the latent variable.\nOn the other hand, discrete latent variable models like Latent Class Analysis (LCA) or Latent Profile Analysis (LPA) assume that individuals are divided into discrete subgroups based on the latent variable, with qualitative differences between groups rather than quantitative. We wouldn’t say that individuals in one group score higher on the latent variable compared to another. Latent class analysis has been used in PTSD, depression, and eating disorder research to uncover groups where individuals exhbit different patterns of behavior from one another, but are not theorized to be less or more severe than other groups.\nMethods that assume a discrete latent variable or a continuous latent variable can both be used to investigate heterogeneity in a sample and to characterize individuals. The question I’ve been grappling with is, how do you know which type of model to use? There is no test or method to determine whether your latent variable is discrete or continuous. Rather, the form of the latent variable is assumed from theory or previous research, and the choice of model will follow. While most researchers just conduct analysis assuming one framework, I’m curious to see what I can learn if I use both continous and discrete latent variable methods on the same dataset. What kind of conclusions can I make, and do conclusions from continuous vs discrete latent variable models contradict or overlap with one another?\nThe dataset that I will be using to carry out my latent variable investigation is the Chinese Longitudinal Healthy Longevity Survey, a national study that assesses quality of life and health status of adults 65 years and older across 22 provinces in China. The goal of the study is to evaluate which biological, social, and environmental factors contribute to longevity and successful aging, and currently has 9 waves spanning from 1998 to 2021. I am interested in indicators related to functional anility. There are 6 items related to ADL: bathing, dressing, toilet, transfer, continence, and feeding. Additionally, there are 8 IADL items: visiting neighbors, shopping, cooking, doing laundry, walking 1km, lift a 5kg weight, crouching/standing, and taking public transportation. For these 14 items participants can respond as either being able to complete the activity independently, able to complete with assistance, or unable to complete. For the purposes of the analysis, ADL and IADL items will be dichotomized so that those who require any assistance will be grouped into one category. I will be using data from the 2005 wave of the study, and for the sake of analysis I will only include elders who have complete data on all of the ADL/IADL items, for a total sample size of 2,637.\nExploratory Factor Analysis (EFA)\n\n\n\nThe goal of EFA is to find the underlying factor structure that explains the correlation between observed items in a dataset. The factor loadings give information about the strength of the relationship between the factor and the item. Based on these factor loadings we can estimate individuals’ factor scores, tell us where they fall along the latent variable continuum. The factor scores can be used in regression or other methods compare various groups on this latent trait.\nTo begin, we must construct a correlation matrix of the items in our dataset. Because these items are all dichotomous, a tetrachoric correlation needs to be estimated before doing EFA. It estimates the Pearson correlation assuming that the binary items are actually coarse approximations of normally distributed latent variable.\n\nCall: tetrachoric(x = dt)\ntetrachoric correlation \n    E1   E2   E3   E4   E5   E6   E7   E8   E9   E10 \nE1  1.00                                             \nE2  0.85 1.00                                        \nE3  0.89 0.94 1.00                                   \nE4  0.87 0.91 0.95 1.00                              \nE5  0.53 0.48 0.61 0.60 1.00                         \nE6  0.85 0.93 0.93 0.86 0.67 1.00                    \nE7  0.77 0.85 0.86 0.83 0.58 0.85 1.00               \nE8  0.77 0.83 0.81 0.83 0.41 0.79 0.90 1.00          \nE9  0.77 0.84 0.81 0.83 0.47 0.79 0.87 0.85 1.00     \nE10 0.74 0.80 0.82 0.78 0.53 0.78 0.80 0.82 0.93 1.00\nE11 0.69 0.78 0.76 0.76 0.37 0.70 0.82 0.85 0.78 0.77\nE12 0.64 0.65 0.76 0.75 0.37 0.62 0.76 0.78 0.74 0.77\nE13 0.63 0.63 0.65 0.70 0.35 0.56 0.70 0.76 0.70 0.71\nE14 0.67 0.74 0.72 0.71 0.24 0.65 0.82 0.84 0.75 0.72\n    E11  E12  E13  E14 \nE11 1.00               \nE12 0.86 1.00          \nE13 0.83 0.86 1.00     \nE14 0.80 0.80 0.81 1.00\n\n with tau of \n   E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11 \n-1.88 -2.35 -2.41 -2.43 -2.53 -2.58 -1.89 -1.39 -1.40 -1.30 -0.97 \n  E12   E13   E14 \n-0.98 -0.76 -0.77 \n\nAfter checking that the ADL/IADL were well correlated with one another, we used common factor analysis with weighted least squares estimation (due to prescence of categorical items). In order to determine the number of factors to estimate I used parallel analysis, which simulates a random dataset with the same number of variables as the existing dataset and extracts eigenvalues. The number of eigenvalues of the existing dataset that are larger than the eigenvalues extracted from the simulated dataset is the number of factors to be estimated.\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \nCall: fa.parallel(x = rr$rho, n.obs = nrow(dt), fa = \"fa\")\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n Eigen Values of \n\n eigen values of factors\n [1] 10.54  0.82  0.18  0.14  0.08 -0.02 -0.05 -0.06 -0.07 -0.12 -0.16\n[12] -0.20 -0.21 -0.32\n\n eigen values of simulated factors\n [1]  0.28  0.10  0.08  0.06  0.05  0.03  0.02  0.00 -0.01 -0.03 -0.05\n[12] -0.07 -0.08 -0.11\n\n eigen values of components \n [1] 10.76  1.18  0.57  0.37  0.29  0.21  0.17  0.14  0.13  0.09  0.06\n[12]  0.03  0.00  0.00\n\n eigen values of simulated components\n[1] NA\n\nBased on parallel analysis I fit a EFA model with 5 factors, along with oblim rotation, a method that allows factors to be correlated with one another and loadings with a more ideal structure. By ideal structure, I mean items that have high factor loadings on only one factor and low loadings on other, allowing for easier interpretation of the factors. For example, say that I fit 2 factors to my set of 14 ADL/IADL items. Perhaps all of my ADL items (feeding, dressing, continence, etc) have high factor loadings on factor 1 and small factor loadings on factor 2. Additionally, my IADL items do the opposite: they have high factor loadings on factor 2 rather than factor 1. This would make interpretation of the factors easier, as we could say that factor 1 represents basic or essential functioning while factor 2 represents more advanced physical functioning. If the continence item had a high factor loading on both factor 1 and 2, it would be less clear on how we would interpret and distinguish these two factors.\n\n\nLoadings:\n    WLS1   WLS2   WLS3   WLS4   WLS5  \nE1   0.813  0.131                     \nE2   0.883  0.115                     \nE3   0.872  0.136                     \nE4   0.871  0.117                     \nE5   0.450  0.473                     \nE6   0.808  0.205                     \nE7   0.881                            \nE8   0.900                            \nE9   0.872                            \nE10  0.856                            \nE11  0.881 -0.136                     \nE12  0.856 -0.166                     \nE13  0.821 -0.193                     \nE14  0.855 -0.194                     \n\n                WLS1  WLS2  WLS3  WLS4  WLS5\nSS loadings    9.808 0.456 0.000 0.000 0.000\nProportion Var 0.701 0.033 0.000 0.000 0.000\nCumulative Var 0.701 0.733 0.733 0.733 0.733\n\nWhile the results from parallel analysis suggest 5 factors, looking at the loadings and proportion of variance explained tells a different story. Factors 2-5 do not have any items with high factor loadings, and the amount of variance among the items that they explain is incredibly low compared to factor 1. Based on these results I decided to restimate the model with only 1 factor instead of 5.\n\n\nLoadings:\n    WLS1 \nE1  0.844\nE2  0.910\nE3  0.904\nE4  0.898\nE5  0.562\nE6  0.856\nE7  0.890\nE8  0.885\nE9  0.873\nE10 0.860\nE11 0.848\nE12 0.816\nE13 0.775\nE14 0.809\n\n                WLS1\nSS loadings    9.929\nProportion Var 0.709\n\nThe results from an EFA model with only one factor estimated show that all 14 ADL/IADL items load onto a single factor. This supports one theory that ADL and IADL items are unidimensional based on this dataset.\nItem Response Theory (IRT)\n\nIteration: 1, Log-Lik: -7073.744, Max-Change: 4.04433Iteration: 2, Log-Lik: -6154.347, Max-Change: 2.72066Iteration: 3, Log-Lik: -5926.419, Max-Change: 1.09802Iteration: 4, Log-Lik: -5825.639, Max-Change: 0.65230Iteration: 5, Log-Lik: -5775.933, Max-Change: 0.34759Iteration: 6, Log-Lik: -5755.767, Max-Change: 0.30668Iteration: 7, Log-Lik: -5743.460, Max-Change: 0.31171Iteration: 8, Log-Lik: -5735.531, Max-Change: 0.29779Iteration: 9, Log-Lik: -5730.198, Max-Change: 0.23537Iteration: 10, Log-Lik: -5725.941, Max-Change: 0.23595Iteration: 11, Log-Lik: -5722.978, Max-Change: 0.23002Iteration: 12, Log-Lik: -5720.663, Max-Change: 0.14865Iteration: 13, Log-Lik: -5719.760, Max-Change: 0.14522Iteration: 14, Log-Lik: -5717.964, Max-Change: 0.07471Iteration: 15, Log-Lik: -5716.556, Max-Change: 0.05848Iteration: 16, Log-Lik: -5715.691, Max-Change: 0.06553Iteration: 17, Log-Lik: -5714.716, Max-Change: 0.05012Iteration: 18, Log-Lik: -5713.921, Max-Change: 0.02543Iteration: 19, Log-Lik: -5713.242, Max-Change: 0.02572Iteration: 20, Log-Lik: -5712.729, Max-Change: 0.01767Iteration: 21, Log-Lik: -5712.310, Max-Change: 0.01392Iteration: 22, Log-Lik: -5711.017, Max-Change: 0.01143Iteration: 23, Log-Lik: -5710.909, Max-Change: 0.00873Iteration: 24, Log-Lik: -5710.809, Max-Change: 0.00877Iteration: 25, Log-Lik: -5710.505, Max-Change: 0.00804Iteration: 26, Log-Lik: -5710.475, Max-Change: 0.00350Iteration: 27, Log-Lik: -5710.450, Max-Change: 0.00413Iteration: 28, Log-Lik: -5710.411, Max-Change: 0.00363Iteration: 29, Log-Lik: -5710.397, Max-Change: 0.00428Iteration: 30, Log-Lik: -5710.385, Max-Change: 0.00206Iteration: 31, Log-Lik: -5710.375, Max-Change: 0.00217Iteration: 32, Log-Lik: -5710.368, Max-Change: 0.00189Iteration: 33, Log-Lik: -5710.361, Max-Change: 0.00154Iteration: 34, Log-Lik: -5710.344, Max-Change: 0.00101Iteration: 35, Log-Lik: -5710.342, Max-Change: 0.00093Iteration: 36, Log-Lik: -5710.340, Max-Change: 0.00087Iteration: 37, Log-Lik: -5710.332, Max-Change: 0.00042Iteration: 38, Log-Lik: -5710.332, Max-Change: 0.00034Iteration: 39, Log-Lik: -5710.331, Max-Change: 0.00032Iteration: 40, Log-Lik: -5710.330, Max-Change: 0.00012Iteration: 41, Log-Lik: -5710.330, Max-Change: 0.00009\n\nThe goal of IRT is to model participants’ probability of responding to a set of items as a function of a hypothesized latent trait. Similar to EFA, we are able to estimate each participant’s latent trait value along a continum and use that value for further comparison and analysis. A core characteristic of IRT is that the model also estimates item parameters that describe the item regardless of the sample used. The parameters depend on the specific IRT model chosen, but could include item difficulty, item discrimination, and guessing parameters. Item difficulty refers to the latent trait needed to have a 50% chance of answering the item correctly (or in non-education settings, the probability of endorsing the item). Items that require a higher latent trait value in order to have that 50% chance have higher difficulty. Item discrimination is a quantity that refers to how well the item differentiates people with similar latent trait values. It can be thought of as a slope. The steeper the slope is, or the larger the item discrimination value is, there’s a higher chance of endorsing the item with every small increment of the latent trait. Guessing parameters are more common in education settings where students may have a chance of randomly selecting the correct answer (like in multiple choice tests) rather than actually being proficient in the latent trait, so this parameter accounts for that possibility.\nIRT is an incredibly useful tool to not only assess the individuals in the sample, but to learn about the characteristics and quality of the items. It is desirable to have items that range in difficulty and have high discrimination so that the survey or assessment overall can assess people with high, low, and inbetween values on the latent continum. This is random but there’s a quote in a fundamental IRT book that I really like “Researchers should spend more time investigating their scales than investigating with their scales.” Using IRT models to evaluate measures is super important because establishing validity is absolutely essential before interpreting results. Oftentimes researchers are eager to use their tools right away without stopping to check if their tools are sharp enough!\nBecause we have some earlier knowledge from fitting the EFA model that these set of ADL/IADL items may be unidimensional, we will fit an IRT model that assumes unidimensionality. There is a 1PL model which only estimates difficulty parameters, which is often the first model that is fit. After fitting the 1PL model I fit a 2PL model which also estimates discrimination parameters. Since this 2PL model was a significantly better fit to the data, I will only discuss the results of the 2PL model. Overall, the fit of the 2PL on this set of 14 ADL/IADL items was generally satisfactory after evaluating some fit statistics. The Root Mean Squared Error of Approximation (RMSEA) was .04, which is below the .05 threshold of adequate fit commonly used in research. In addition, the Comparative Fit Index (CFI) was .99, above the well-known criteria of .95. These fit statistics never guarantee fit or that the best model is chosen, but provide evidence to support its continued use and interpretation of its parameters.\n\n           a          b g u\nE1  2.682206 -2.2727847 0 1\nE2  4.086769 -2.6274767 0 1\nE3  4.901545 -2.6476169 0 1\nE4  4.485009 -2.7009171 0 1\nE5  1.394256 -4.4036514 0 1\nE6  3.649285 -3.0013932 0 1\nE7  4.019452 -2.0609180 0 1\nE8  4.417876 -1.4785459 0 1\nE9  3.896057 -1.5269327 0 1\nE10 3.517278 -1.4424198 0 1\nE11 4.328125 -1.0458542 0 1\nE12 3.869158 -1.0694429 0 1\nE13 3.453457 -0.8526636 0 1\nE14 3.305270 -0.8660019 0 1\n\nTable X above shows the discrimination (a) and difficulty (b) estimates for this dataset. The least difficult item is continence, whether one is able to use the restroom without assistance. One does not need to be on the high end of this latent trait spectrum, in this case the latent trait being functional ability, in order to have a 50% chance of being able to independently do this item. This estimate tracks with general gerontology research which shows that loss of continence is usually the last functional limitation that occurs and usually signals severe health decline. Other low difficulty items are feeding and getting dressed. On the other hand, the most difficult items are being able to crouch and stand 3 times as well as taking public transportation. Individuals need to have higher functional ability in order to have a higher probability of being able to do this activity.\n\n\n\nThe figure above shows information curves for each of the 14 items. These information curves let us know how informative these items are at different values of the latent trait, represented by theta. The higher the peak, the more information this item provides for people who are at that theta level. Both the item discrimination and the item difficulty are used to construct these information curves, and one can see that different items provide information for different values of the latent trait. For example, E5 (continence) provides the most information for people who are low on functional ability. This makes sense intuitively if you think about the fact that people with higher functional ability are all able to use the bathroom independently, so this item is not able to distinguish people who are past a certain threshold of basic functional ability. On the other hand, E14 (using public transportation) provides the most information for people who have higher functional ability. People with low functional ability who struggle to use the bathroom or get dressed are all going to be not able to use public transportation, so this item is not very helpful at differentiating people with a lower range of functional ability.\n\n   item   S_X2 df.S_X2 RMSEA.S_X2 p.S_X2\n1    E1  7.737       8      0.000  0.460\n2    E2  1.258       2      0.000  0.533\n3    E3  6.709       2      0.030  0.035\n4    E4  1.984       2      0.000  0.371\n5    E5  5.710       7      0.000  0.574\n6    E6  2.807       1      0.026  0.094\n7    E7  7.342       6      0.009  0.290\n8    E8 19.749       6      0.029  0.003\n9    E9  4.483       6      0.000  0.612\n10  E10  7.652       6      0.010  0.265\n11  E11  4.594       4      0.008  0.332\n12  E12  4.776       5      0.000  0.444\n13  E13 21.265       5      0.035  0.001\n14  E14 14.664       5      0.027  0.012\n\nWhile the overall model of fit is adequate, it is also possible to run item-level fit statistics to identify items that are not performing as expected based on the model and that could be reviewed. Large S-\\(\\chi^2\\) values with significant values indicate items that deviate significantly from the 2PL model. In this example we have two poor-fitting items using a .05 threshold: Toileting and shopping independently. If one was using IRT for scale development and to add/remove items, it is recommended to review these item fit statistics and do further analysis to investigate whether these two items should be excluded or not. Since my focus here is not scale development I will not proceed with any further adjustments.\n\n\n\nLastly, I want to turn our focus on the person parameters. Based on the estimated item parameter values, we can also estimate latent trait scores for each person in the dataset. The graph above shows the distribution of theta scores for this sample of older Chinese adults. Overall, most adults in the sample have relatively higher functional ability, while there are a smaller subset that vary in terms of severity of functional disability. The actual numerical value of the theta scores should not be interpreted, but can be used to compare comparisons within the sample. This distribution aligns with the sample characteristics, as these older adults have been sampled from the general population and excludes institutionalized older adults who may have severe functional limitations.\nOverall, using IRT on this data shed some light about which items are informative for describing individuals that are located on different parts of the functional ability variable. While we obtain more information about our items compared to EFA, this is more of a secondary step after EFA rather than a competing model during step 1 of data exploration. In order to fit the 1PL and 2PL IRT models, the assumption of unidimensionality must be justified. Results from our EFA where we allowed for the possibility of multiple factors to explain the correlation of our items indicated that one factor was sufficient for explaining our data, so thus I decided to fit these IRT models. The distribution of latent trait scores from both EFA and the 2PL model were very similar. My analysis here stops short but a next step I could take is to use these latent trait scores as a predictor for some type of outcome. In the context of aging it could be a useful predictor of depression and mental wellbeing (although the direction of this model is kind of undetermined because mental wellbeing could also predict functional ability too..). I don’t do gerontology research or have any strong hypotheses so I will just stop here and move on to the discrete variable case: latent class models!\nLatent Class Analysis (LCA)\n\n\n\n\n\n\nConclusion\n\n\n\n",
    "preview": "posts/2025-07-28-discrete-continous-LV/2025-07-28-discrete-continous-LV_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-07-28T23:26:58-07:00",
    "input_file": "2025-07-28-discrete-continous-LV.knit.md"
  },
  {
    "path": "posts/2025-07-19-poLCA/",
    "title": "poLCA: Starting values for Class Prevalence Probabilities",
    "description": "Sometimes maybe less is more..",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-07-19",
    "categories": [
      "research"
    ],
    "contents": "\nIntroduction\nThis post assumes familiarity with latent class models. [Here] is an intro if you aren’t.\nWhen estimating latent class models, starting values are needed to get the estimation algorithm going. When I first learned about latent class models, I came across this R package poLCA which allows for latent class models to be estimated very easily. After 30 minutes of poking around I was able to use the package to estimate a simple latent class model. All of the parameter estimates and model fit information are easy to access and you can even plot the class solutions in a 3D bar graph using one of the functions in the package!\nIt wasn’t until a year or so later during graduate school where I became interested in estimation. Expectation-maximization (EM) algorithm is the most common algorithm for estimating latent class models, and is what is used in the poLCA package. After understanding the basic gist of the EM algorithm I was always curious about how it was actually implemented, but other software like STATA and MPLUS do not have publicly accessible code. That’s why I started looking into the code for poLCA which is open-source and able to be viewed directly on github. That’s when I noticed that while there was code to randomly generate item response probabilities from a uniform distribution or provide user input for item response probabilities, there wasn’t that option for the latent class prevalence probabilities. The default was that the starting latent class prevalence probabilities were evenly split by the number of classes estimated. For example, if 4 classes are estimated, then the starting value probabilities for each class is .25.\nMotivation\nThat got me thinking…would this default specification of equal latent class sizes for the starting value have an impact on estimation? Around this time I was starting to think about how estimation of latent class models could vary: what makes them easy or challenging to estimate? Conversations with my advisor helped me develop some hypotheses about challenging estimation conditions such as small sample sizes and items with high measurement error. Unbalanced latent class proportions came up as a factor worth examining, since smaller classes means that there are fewer observations in the dataset that will contribute information to the model when estimating parameters for that smaller class.\nHaving unbalanced latent class prevalences (such as 4 classes with prevalences of .75, .12,.08,.05) is common in alcohol research, where high or frequent substance use represents a smaller proportion of the population. In a simulation study examining the impacts of misspecification on class enumeration, the authors found that compared to data generated from a two class model with 50%/50% membership in the two classes, class enumeration indices (used for deciding the optimal number of classes to fit the data) performed worse when the two classes had a 80%/20% membership instead. This research shows support for the idea that unbalanced class sizes impact the estimation process and the final solution.\nNow returning back to the issue of starting values. Say we have data generated from a latent class with unbalanced class sizes, making it very likely to have a complicated likelihood surface (aka more challenging to find the best parameters that describe the data). If we start our estimation algorithm with starting values where the classes are equally sized, will the algorithm struggle to find the optimal parameter values? When estimation latent class models we most often follow the principles of maximum likelihood estimation. Given our observed data that we have (response patterns to a set of items), we choose the set of parameter values, item response probabilities and latent class prevalences, that maximize the likelihood of observing the data. I like to think of the likelihood as a vast landscape with lots of hills and valleys. We want to find the location of the highest peak, which represents the set of parameter values that correspond to the best likelihood, but we might get stuck at a lower peak and call it a day there instead. These estimation algorithms might stop at what is believed to be the highest peak, but it reality it is not. This is called reaching a local solution, rather than the maximum solution. So my question is, by starting our algorithm in a location that is a bit further from the true parameter values (aka further away from the highest peak), will the algorithm be more likely to land on a local solution? If that is the case, then maybe we could propose a way to allow for unbalanced class size estimates to be used as starting values rather than only equal class sizes.\nWhat I Did\nWhat I did was quite simple. I copied the main poLCA function from the package’s github and modified a small chunk of the code so that the user could specify whatever latent class prevalence values they wanted for the initialization of the algorithm, rather than only always using equal class sizes. Here’s what I wanted to test: if I gave the same dataset and item response probability starting values to two versions of poLCA, one where the latent class starting values are equal, and one where they are not equal, would they both reach the same solution?\nI only used one dataset in this example, and so more datasets should be done in order to have a definitive answer, but nonethless I think these initial results can be illuminating.\n\n\n\nThe graph above shows the parameters used to generate the dataset. These are the true values that we hope to recover from fitting our model. Each of the bars represents the probability of saying “no” to a particular item. The graph in the upper left hand corner shows the solution for class 1. .05 is the latent class prevalence, this means that 5% of our sample is estimated to be part of this class. Based at looking at the bars, in this class individuals have a 35% of saying “no” to all items X1-X10 in this simulated fake data example. If we translate this to a substance use example just for kicks, let’s say that items X1-10 ask whether someone has consumed different types of substances such as alcohol, marijuana, cocaine, etc., with one drug for each question. Because individuals in this class have a low probability of saying no to each of these items, we might label this class as “high substance use”. This class only represents 5% of our population, which could track with what we see in real world research with smaller populations of poly-substance users. The class on the bottom right would represent “low substance use”, as shown by high probability of saying “no” to all of the items X1-X10.\nThis class represents 70% of the entire sample, which is a large majority! The other two classes show varying response patterns and highlight the heterogeneity in responses.\nResults\n\n\n\nSo I ran two models. One model with equal latent prevalence probabilities (.25,.25,.25,.25) for starting values, and the other model with unequal latent prevalence probabilities (.7,.15,.1,.05). Both models had the exact same starting values for the item response probabilities. The results after estimation are shown in the figure below. Based on just looking at the top most graph on both the equal and unequal models, we can see some differences in the final solution. When using equal class probabilities as the starting value, we see that the chance of saying “no” for item X1 is 56%, while on the other hand when we use unequal class probabilities, the chance of saying “no” for item X1 is 37%. This difference in parameter values would change the interpretation of the class. The first solution suggests individuals in this class are likely to say “no” for item X1, while the second solution suggests the opposite! A similar situation arises for items X9 and X10 for the same class! Notice that these large differences between start value method only occurs in the smallest latent class. This goes back to what I mentioned earlier about smaller classes being harder to estimate due to limited information!\nWhich solution is better? Based on looking at the likelihood value, where we aim to choose the solution with the largest likelihood value, we would go with the solution produced by starting with equal latent class prevalence probabilities. Does this mean all of this was moot and that starting with equal probabilities is always the way to go? Not yet…\nWhen fitting latent class models, the issue of algorithms getting stuck on local solutions rather than the maximum is well known. Thus, textbooks and articles about best practices for latent class models frequently suggest running many iterations with varying starting values, and then choosing the solution with the largest likelihood value. So given that my earlier exploration was just one iteration, let’s ramp it up to 1000 iterations. For each method of starting values, equal and unequal latent class prevalence probabilities, we will run the model 1000 times (making sure that both methods have the same set of 1000 sets of item response probability starting values), and choose the best solution for each of the methods and compare. This way aligns more closely with how researchers would actually fit these models. It is possible that despite some differences in solutions, the overall best solution found by the two methods over 1,000 iterations might just be the same.\n\n\n\nLo and behold, the best solution found by both methods over 1,000 iterations yielded the same solution! There are some slight differences in the parameter estimates but they are beyond the thousandths decimal place so for practicality I will say that they’re the same solution. So that’s interesting! One important thing I wanted to highlight is that these solutions are pretty far away from the true data generating parameters! Despite the similarity in the final solution between the two methods, these results highlight the difficulties of estimating this model with this dataset. This dataset not only has item response probabilities with high measurement error, but only has a sample size of 400, which is smaller than the recomended number for fitting latent class models!\nWe’ve established in this example that the overall final solution doesn’t differ much, but I’m still curious about whether there are some differences iteration by iteration between the two methods. If so, is there one method that tends to find better solutions over the other? I made two tables that compare how many times one method was superior to the other when they both used the exact same set of item response probability starting values over the 1,000 iterations.\n\n\nTable 1: Comparing Convergence Between Start Value Methods\n\n\nDescription\n\n\nValue\n\n\n% of iterations where both fail to converge\n\n\n14\n\n\n% of iterations where equal class probabilities fail but unequal doesnt\n\n\n19\n\n\n% of iterations where unequal class probabilities fail but equal doesnt\n\n\n24\n\n\nBefore we can compare solutions, we should look at convergence first. Not all iterations will successfully converge. Failure to converge could be due to the location of the starting values causing the algorithm to get stuck in an area where it cannot improve. As shown in Table 1, we can see that using unequal latent class prevalence probabilites as starting values resulted in slightly higher convergence failures compared to using equal probabilities.\n\n\nTable 2: Comparing Solutions Between Start Value Methods\n\n\nDescription\n\n\nValue\n\n\n% of iterations where equal and unequal class probabilities have same solution\n\n\n44\n\n\n% of iterations where unequal class probabilites has better solution\n\n\n24\n\n\n% of iterations where equal class probabilites has better solution\n\n\n32\n\n\nTable 2 compares the solutions found by the two start values method. A better solution in this case is defined by the solution with the higher likelihood value. Almost half of the iterations resulted in the two methods converging to the same solution. When they didn’t, the equal probabilities starting values was slightly more likely to find a better solution.\nConclusion\nI don’t think my work in this area is done yet, but the results that I have found so far are interesting. Examining results on a granular level show some differences in solutions when using equal latent class prevalence probabilities as starting values for the algorithm versus using unequal latent class prevalence probabilities. However, the overall best solution out of 1,000 iterations produced by the two methods are the same. Further data conditions and simulations should be run before any strong conclusions are made, but from my perspective I believe the poLCA package should allow users to specify different latent class prevalence probabilities as starting values as an option. Even if it does not ensure a better likelihood solution is found, it could help ensure that the complicated likelihood surface is adequately explored in all of its nooks and crannies (in a high dimensional sense..).\nOverall this was super fun to explore and I’m glad I took the time to do a deep dive on an existing package’s code! My takeaway message I hope to convey is that estimation is extremely hard and we should thoroughly evaluate all the decisions we make during the estimation process, whether that is how we generate starting values or even which algorithm we choose!\nIf you’ve made it this far thanks for reading.\n\n\n\n",
    "preview": "posts/2025-07-19-poLCA/Rplot1.png",
    "last_modified": "2025-07-23T00:50:56-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-07-18-welcome-again/",
    "title": "My First Post (PhD verison)",
    "description": "It's been 4 years since my last post! I still don't know how to use git",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2025-07-18",
    "categories": [
      "food",
      "research"
    ],
    "contents": "\nHowdy! It’s been a while since I last tried to start up this blog again. I want to practice writing up and sharing some of my research I’ve been working on for the past 3 (!!!!) years. The last time I wrote here I was still a wee little undergraduate student who had 0 idea about grad school and alas… here we are.\nIt took me a long time to get to the research topics that I’m interested in now. I started my program not even knowing what latent class models are, so I’d like to think I worked myself from the ground up! Broadly, I am interested in digging deeper into measurement models that psychology researchers use. My own personal theory is that the ease of software allows researchers to run extremely complex models with just one line of code and return results in a matter of seconds, leading many to believe that the models are easy to compute and extract values. Most of the time these models are extremely challenging to estimate, and software programs makes a lot of choices that researchers don’t about, which can impact their results! One choice in particular is the estimation algorithm used to obtain parameter estimates. My current work, which I will detail in a separate post, compares how two different estimation algorithms result in different solutions for the same model and same dataset!\nThe model I’m focusing on at the moment is latent class models! Latent class models aim to characterize individuals in a sample into different groups, where each group differs on some unobserved (latent) variable. Latent class models have been used in substance research, education, and so much more. Pretty much every field I’ve encountered in research will have at least 1-2 papers using latent class models. Latent class models differ from machine learning methods that also aim to classify people into groups (k-means clustering) in that the items or predictors are assumed to have measurement error. We assume that the thing (or latent variable) that distinguishes different groups is unobservable, and that our items are just an approximation of the real thing. Examples include depression, alcohol dependence, burnout, and disordered eating.\nMore specifically, here are some of the topics I’m interested in and what I hope to write articles on…\nDo Newton-Raphson and Expectation-Maximization Algorithms traverse the latent class likelihood surface in the same way?\nDo different types of parameter restrictions (equality, inequality, bounds) impact estimation of latent class models in different ways?\nSome less fleshed out research ideas include..\nSimulated annealing + genetic algorithms and how they compare to traditional algorithms\nstart value quality\nMy first article will be how starting values are generated for a popular R package poLCA. Please stay tuned.\nOf course it wouldn’t be me if I didn’t talk about food..I wanted to highlight a new spot in Seattle I really like called Many Bennies. They serve New Zealand style ice cream where they blend your choice of fruits with a sweet cream base! It tastes like no other ice cream that I’ve ever had before. It has the lightness of frozen yogurt but it’s creamy and not tart at all! After eating this ice cream I realize how insanely sweet traditional ice cream places are…I’m not even a huge fruit fan but after trying this place I can never go back to Ben and Jerrys or Molly Moons haha. I got blueberries and strawberries and it was the perfect combination. They also give free lactaid which is super cool!\n\n\n\nIf you made it this far, thanks for reading. Take care!\nJessica\n\n\n\n",
    "preview": {},
    "last_modified": "2025-07-19T00:44:13-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-my-first-post/",
    "title": "My First Post",
    "description": "I'm really struggling with the distill package! And git commands!",
    "author": [
      {
        "name": "Jessica",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nI spent about 1 hour deleting and cloning and merging different repositories.. instead of properly learning git commands I’ve been relying on the integrated git feature in RStudio.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-07-18T23:03:35-07:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Crouton Thoughts",
    "description": "The world needs to hear what I have to say!",
    "author": [
      {
        "name": "Jessica",
        "url": "https://github.com/jeyu22"
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\n\n\n\nHi! My name is Jessica. I’m a senior at Amherst College studying statistics and psychology. I made this blog for fun in hopes of motivating myself to be a more active programmer and data scientist! I love R and #TidyTuesday. Besides statistics I enjoy finding new places to eat in the D.C/Maryland/Virginia area.\nHere’s a list (mainly for myself) of posts I want to make in the next few months:\nRatings of all the restaurants/cafes I’ve tried (ideally on a leaflet plot…)\nFood reviews…\nTidy Tuesday challenges\nSide projects..\nI’m also writing a statistics thesis this upcoming fall, so hopefully I’ll post updates as that progresses!\nWhy croutons?\nThey’re versatile: as a topping for salad, a tasty snack, or an entire meal. My favorite croutons are the ones that come with the caesar salad at Costco.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-07-18T23:24:33-07:00",
    "input_file": {}
  }
]
